%!TEX root = pset2.tex

\section{Support Vector Machine}\label{sec:svm}

Support Vector Machines are a popular classification method to construct linear or nonlinear decision boundaries
by solving a convex optimization problem.  There are two common forms of the optimization problem considered for SVM,
which we refer to as the primal and dual.  In this paper, we only consider the dual form, because it is computationally more tractable
for many problems, and this method has the ability to generalize to different choices of kernel.  The dual form of SVM for a general
kernel function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is as follows:

\begin{equation}
\label{eq:svm_dual}
\begin{array}{rll}
\underset{\alpha \in \mathbb{R}^n}{\max}~ & \sum\limits_{i = 1}^n \alpha_i 
- \frac{1}{2} \sum\limits_{i = 1}^n\sum\limits_{j = 1}^n \alpha_i \alpha_j y^{(i)} y^{(j)} k(x^{(i)}, x^{(j)}) \vspace{3pt}\\
\textup{s.t.}~ & 0 \leq \alpha_i \leq C,&~~~i=1,\ldots ,n, \vspace{3pt}\\
& \sum\limits_{i = 1}^n \alpha_i y^{(i)} = 0. & \\
\end{array}
\end{equation}

\subsection{Implementation}

First, we implemented the dual form of the SVM with a linear kernel, where $k$ is the usual dot product $k(x, z) = \langle x,z \rangle$ for all $x, z \in \mathcal{X}$. 
In MATLAB, we created a function with inputs: data $X \in \mathbb{R}^{n \times p}$, labels $Y \in \{-1,1\}$, and cost parameter $C \in \mathbb{R}^{+}$. 
Within the function, we use the quadratic solver \texttt{quadprog} to solve the SVM dual problem (\ref{eq:svm_dual}) with these parameters to find the optimal $\alpha$'s.
Since \texttt{quadprog} requires that the problem fit into a certain functional form, we reformulate the problem (\ref{eq:svm_dual}) as follows:

\begin{equation}
\label{eq:svm_dual2}
\begin{array}{rll}
-\underset{\alpha \in \mathbb{R}^n}{\min}~ & \frac{1}{2} \alpha^T H \alpha - \sum\limits_{i = 1}^n \alpha_i \vspace{3pt}\\
\textup{s.t.}~ & 0 \leq \alpha_i \leq C,&~~~i=1,\ldots ,n, \vspace{3pt}\\
& \sum\limits_{i = 1}^n \alpha_i y^{(i)} = 0, & \vspace{5pt}\\
\end{array}
\end{equation}
where:  $H \in \mathbb{R}^{n \times n}$ is a matrix with $(i,j)^{th}$ entry $H_{ij} = y^{(i)} y^{(j)} k(x^{(i)}, x^{(j)})$.  Given the optimal solution $\alpha \in \mathbb{R}^n$ for the SVM problem with a linear kernel, the chosen linear decision boundary $\theta^T x + \theta_0 = 0$ is given by:

\begin{equation}
\label{eq:svm_theta}
\theta = \sum\limits_{i = 1}^n \alpha_i y^{(i)} x^{(i)}
\end{equation}
\begin{equation}
\label{eq:svm_theta_0}
\theta_0 = \frac{1}{\mathcal{M}} \Big(\sum\limits_{j \in \mathcal{M}} \Big(y^{(j)} - \sum\limits_{i \in \mathcal{S}}\alpha_i y^{(i)} (x^{(j)})^T x^{(i)} \Big)\Big)
\end{equation}

The output of our linear SVM function is $[\theta, \theta_0]$.  We tested our function on the 2D example $X = \{(1,2),(2,2),(0,0),(-2,3)\}$, $Y = \{1,1,-1,-1\}$.  
For this problem, the objective function generated for problem (\ref{eq:svm_dual2}) is:

\begin{equation}
\label{eq:svm_objective1}
\frac{1}{2} \alpha^T H \alpha - \sum\limits_{i = 1}^4 \alpha_i,
\end{equation}
where:
\[
H = \begin{bmatrix}
     5   &  6  &   0  &  -4 \\
     6   &  8  &   0  &  -2 \\
     0   &  0  &   0  &   0 \\
    -4   & -2  &  0  &   13 \\
\end{bmatrix}
\]

The constraints are:
%
\begin{equation}
0 \leq \alpha_i \leq C,~~~i=1,\ldots ,4,
\end{equation}
\begin{equation}
\alpha_1 + \alpha_2 - \alpha_3 - \alpha_4 = 0.
\end{equation}

\subsection{Performance on datasets}

We tested our linear SVM function on the same 2D datasets from the previous section, with parameter $C = 1$.  

% TODO

\subsection{Kernel SVM}

We extended our SVM implementation in MATLAB to operate with more general kernels, taking the kernel function or kernel matrix as input.  

{\bf Questions:}
\begin{enumerate}[label=(\alph*)]
	\item As $C$ increases, the geometric margin $1/\|\V{w}\|$ decreases.  If the data is not linearly separable, then
	the geometric margin $1/\|\V{w}\|$ decreases strictly monotonically as $C$ increases.  However, if the data is linearly
	separable, then this does not always happen as we increase $C$.  In this case, once the margin is sufficiently small
	such that all points are correctly classified, then it will not decrease further even as $C$ approaches infinity.  
	
	\item As $C$ decreases, the number of support vectors decreases.  This is because the larger penalty on misclassified
	points leads to a decision boundary with fewer misclassifications on the training data.  However, the number of the
	support vectors is bounded below by two as $C$ approaches infinity, because there will always be at least one support vector
	on each side of the decision boundary.
	
	\item Maximizing the geometric margin $1/\|\V{w}\|$ on the training data is not an appropriate criterion for selecting
	$C$ because this leads to a classifier which is overfit to the training set.  To obtain a classifier which generalizes well
	on test data, we should use out-of-sample data to select an appropriate value for $C$.  To do this, we can train the SVM model
	with $C = \{0.01,0.1,1,10,100\}$, and then select the value for $C$ which yields the classifier with the highest
	accuracy on the validation set.  

\end{enumerate}

%TODO: continue
%!TEX root = pset1.tex

\section{Ridge Regression}\label{sec:ridge_reg}

\subsection{Implementation}
Ridge regression is the particular case of regularized least squares with a quadratic regularizer term.  The error function that we aim to minimize over is given by:
\begin{equation} \label{eq:ridge_error_fn}
\frac{1}{2} \sum_{n=1}^{N} (t_n - \M{w}^T \phi (\M{x}_n) )^2 + \frac{\lambda}{2} \M{w}^T \M{w}
\end{equation}

The closed-form solution of this problem is well-known, and can be derived by setting the gradient of (\ref{eq:ridge_error_fn}) equal to zero.  The optimal solution for $\M{w}$ is provided by Bishop (2006), page 145:

\begin{equation} \label{eq:ridge_sol}
\M{w}_{ridge} = (\lambda \M{I} + \Phi^T \Phi)^{-1} \Phi^T \M{t}
\end{equation}

We coded this method in MatLab and tested our program using data from Bishop Figure 1.4, varying the parameters of $\lambda$ and $M$.  For the extreme cases, we observed that if $\lambda \leq 0.0001$, then $\M{w}_{ridge} \approx \M{w}_{OLS}$, and if $\lambda \geq 100$, then $\M{w}_{ridge} \approx \M{0}$.

% Reference: Bishop (2006), pages 144-145

% TODO: add plots
% TODO: add description of results when M = 1, 3, 5

\subsection{Model Selection}
To optimize parameter values for $\lambda$ and $M$, we build our models using training data and then compare out-of-sample performance on validation data.  For this example, we performed a grid search over the ranges: $\lambda = \{0.0001, 0.001, 0.01, 0.1,1,10,100\}$, $M = \{0,1,2,3,4,5\}$.  We found that the model with $\lambda = $ TODO and $M = $ TODO yields the lowest MSE on validation data, so we select these to be the final parameter values.  This model leads to MSE = TODO for the test data, which is the TODO lowest overall.  In general, we observe that models with MSE for the validation set tend to yield low MSE on the test set.  

% TODO: add numbers for above paragraph
% TODO: add tables with x-axis M, y-axis  lambda, entries MSE for validation and test data (2 tables)

\subsection{Blog Feedback}
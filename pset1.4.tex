%!TEX root = pset1.tex

\section{Sparsity and LASSO}\label{sec:gen}
The data for this problem is generated according to  $f(x) = \M{w}^T \phi (\M{x}) + \epsilon$, where $\epsilon \sim  N(0,\sigma^2)$.  For this problem, we assume that $\M{w}$ is unknown, and we estimate this vector with ridge regression in Section~\ref{sec:sparsity_ridge_reg} and with LASSO in Section~\ref{sec:sparsity_grad_desc}.  Let $\lambda_2$ denote the regularizer term for the $L_2$ norm and let $\lambda_1$ denote the regularizer term for the $L_1$ norm.  

\subsection{Ridge Regression Gradient Descent} \label{sec:sparsity_ridge_reg}
Applying our method for gradient descent to the ridge regression problem on the training dataset of 12 points, we compute the vector of weights $\M{w}$ and MSE on the testing dataset for $\lambda_2 = 0.1$.  We obtain: 
%
\begin{flalign*}
\M{w} &= (w_1, w_2, w_3, w_4, w_5, w_6, w_7, w_8, w_9, w_{10}, w_{11}, w_{12}),\\
\text{MSE} &= TODO.
\end{flalign*}
%
If we disable the $L_2$ regularizer (set $\lambda_2 = 0$), we obtain:
%
\begin{flalign*}
\M{w} &= (w_1, w_2, w_3, w_4, w_5, w_6, w_7, w_8, w_9, w_{10}, w_{11}, w_{12}),\\
\text{MSE} &= TODO.
\end{flalign*}
%
\subsection{LASSO Gradient Descent} \label{sec:sparsity_grad_desc}
Similarly, we apply our method for gradient descent to the LASSO problem on the on the training dataset of 12 points.  For $\lambda_1 = 0.1$, we find: 
%
\begin{flalign*}
\M{w} &= (w_1, w_2, w_3, w_4, w_5, w_6, w_7, w_8, w_9, w_{10}, w_{11}, w_{12}),\\
\text{MSE} &= TODO.
\end{flalign*}
%
If we disable the $L_1$ regularizer (set $\lambda_1 = 0$), we find:
%
\begin{flalign*}
\M{w} &= (w_1, w_2, w_3, w_4, w_5, w_6, w_7, w_8, w_9, w_{10}, w_{11}, w_{12}),\\
\text{MSE} &= TODO.
\end{flalign*}
%
\subsection{Sparsity Properties of LASSO}